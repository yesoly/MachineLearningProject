{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_10.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLIkD+2u1PaY9PMcCZgG3H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yesoly/MachineLearningProject/blob/master/Assignment_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx7bjb1zOwMm"
      },
      "source": [
        "# Optimal Selection of the hyper-parameters associated with the classification on MNIST\n",
        "Choose an optimal set of hyper-parameters and design a neural network for the classification of MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXLdTm3EQLEm"
      },
      "source": [
        "import os\n",
        "\n",
        "# load data\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# train\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "\n",
        "# visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7EUEJRYRMMn"
      },
      "source": [
        "check device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp8BdsB8RH06"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device: {}'.format(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoLzwBZTO3_w"
      },
      "source": [
        "## 1. Data\n",
        "* you can use any data normalisation method\n",
        "* one example of the data normalisation is whitenning as given by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-4s2n2DkIK3"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVIk29VmPBl6"
      },
      "source": [
        "* load the MNIST dataset\n",
        "* use the original training dataset for testing your model\n",
        "* use the original testing dataset for training your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FESTOm1nOv7e"
      },
      "source": [
        "data_path = './MNIST'\n",
        "\n",
        "data_test   = datasets.MNIST(root = data_path, train= True, download=True, transform= transform_test)\n",
        "data_train  = datasets.MNIST(root = data_path, train= False, download=True, transform= transform_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEI8L3lIPIk1"
      },
      "source": [
        "* Note that the number of your training data must be 10,000\n",
        "* Note that the number of your testing data must be 60,000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwHjktlEOv1Q"
      },
      "source": [
        "print(\"the number of your training data (must be 10,000) = \", data_train.__len__())\n",
        "print(\"hte number of your testing data (must be 60,000) = \", data_test.__len__())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYeRoqwGPMvq"
      },
      "source": [
        "## 2. Model\n",
        "\n",
        "* design a neural network architecture with three layers (input layer, one hidden layer and output layer)\n",
        "* the input dimension of the input layer should be 784 (28 * 28)\n",
        "* the output dimension of the output layer should be 10 (class of digits)\n",
        "* all the layers should be fully connected layers\n",
        "* use any type of activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiBNYfzZOqM9"
      },
      "source": [
        "class classification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(classification, self).__init__()\n",
        "        \n",
        "        # construct layers for a neural network\n",
        "        self.classifier1 = nn.Sequential(\n",
        "            nn.Linear(in_features=28*28, out_features=512),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.2),\n",
        "        ) \n",
        "        self.classifier2 = nn.Sequential(\n",
        "            nn.Linear(in_features=512, out_features=512),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.2),\n",
        "        ) \n",
        "        self.classifier3 = nn.Sequential(\n",
        "            nn.Linear(in_features=512, out_features=10),\n",
        "            nn.ReLU(inplace = True),\n",
        "        ) \n",
        "    \n",
        "    def forward(self, inputs):                 # [batchSize, 1, 28, 28]\n",
        "        x = inputs.view(inputs.size(0), -1)    # [batchSize, 28*28]\n",
        "        x = self.classifier1(x)                # [batchSize, 20*20]\n",
        "        x = self.classifier2(x)                # [batchSize, 10*10]\n",
        "        out = self.classifier3(x)              # [batchSize, 10]\n",
        "        \n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUjvEAvoPXEd"
      },
      "source": [
        "## 3. Loss function\n",
        "* use any type of loss function\n",
        "* design the output of the output layer considering your loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28p5aYttPY4V"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4FsFtk9Pck9"
      },
      "source": [
        "## 4. Optimization\n",
        "* use any stochastic gradient descent algorithm for the optimization\n",
        "* use any size of the mini-batch\n",
        "* use any optimization algorithm (for example, Momentum, AdaGrad, RMSProp, Adam)\n",
        "* use any regularization algorithm (for example, Dropout, Weight Decay)\n",
        "* use any annealing scheme for the learning rate (for example, constant, decay, staircase)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIB3LTP7WHV4"
      },
      "source": [
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reBkM6fZWGR5"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVndxE3BSSUN"
      },
      "source": [
        "model = classification()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWxEtzMPPoeL"
      },
      "source": [
        "LEARNING_RATE = 0.0015\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaUXG2qrR0Oc"
      },
      "source": [
        "##5. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1ewx1DdR2ok"
      },
      "source": [
        "epochs = 15\n",
        "test_loss_min = np.Inf\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "history_accuracy = []\n",
        "history_running_acc = []\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        model.train()\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        ps = model(images)\n",
        "        _, top_class = ps.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "\n",
        "        loss = criterion(ps, labels)\n",
        "        running_acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "\n",
        "    else:\n",
        "        test_loss = 0\n",
        "        accuracy = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model.eval() \n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                ps = model(images)\n",
        "                _, top_class = ps.topk(1, dim=1)\n",
        "                equals = top_class == labels.view(*top_class.shape)\n",
        "                \n",
        "                test_loss += criterion(ps, labels).item()\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "      \n",
        "        train_losses.append(running_loss/len(train_loader))\n",
        "        test_losses.append(test_loss/len(test_loader))\n",
        "        history_accuracy.append(accuracy/len(test_loader))\n",
        "        history_running_acc.append(running_acc/len(train_loader))\n",
        "\n",
        "    print(f\"Epoch: {e}/{epochs}.. \",\n",
        "          f\"Training Loss: {running_loss/len(train_loader):.3f}.. \",\n",
        "          f\"Testing Loss: {test_loss/len(test_loader):.3f}  / \",\n",
        "          f\"Train Accuracy: {running_acc/len(train_loader):.3f}  \",\n",
        "          f\"Test Accuracy: {accuracy/len(test_loader):.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPP0pCvDajxh"
      },
      "source": [
        "## 6. Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORT0ckK1agxv"
      },
      "source": [
        "1. Plot the training and testing losses over epochs [2pt]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO1cai7iYlSL"
      },
      "source": [
        "fig_1 = plt.figure(figsize=(8,8))\n",
        "plt.plot(np.array(range(epochs)), train_losses, c = 'r', label = 'Training Loss')\n",
        "plt.plot(np.array(range(epochs)), test_losses, c = 'b', label = 'Test Loss')\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.title('Plot the loss curve')\n",
        "plt.show()\n",
        "fig_1.savefig('loss curve.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGasHySTanSE"
      },
      "source": [
        "2. Plot the training and testing accuracies over epochs [2pt]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B28eBEZ7Yxxm"
      },
      "source": [
        "fig_2 = plt.figure(figsize=(8,8))\n",
        "plt.plot(np.array(range(epochs)), history_running_acc, c = 'r', label = 'Train Accuracy')\n",
        "plt.plot(np.array(range(epochs)), history_accuracy, c = 'b', label = 'Test Accuracy')\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.title('Plot the accuracy curve')\n",
        "plt.show()\n",
        "fig_2.savefig('accuracy curve.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFyys6x8apOT"
      },
      "source": [
        "3. Print the final training and testing losses at convergence [2pt]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_00IdpyPataG"
      },
      "source": [
        "result_loss = pd.DataFrame({'loss':[train_losses[-1], test_losses[-1]]}, index = ['training loss','testing loss'])\n",
        "result_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "airXiaypat3t"
      },
      "source": [
        "4. Print the final training and testing accuracies at convergence [20pt]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZXUUP6NauLJ"
      },
      "source": [
        "result_acc = pd.DataFrame({'accuracy':[history_running_acc[-1].item(), history_accuracy[-1].item()]}, index = ['training accuracy','testing accuracy'])\n",
        "result_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i9TQxoTPouU"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOuVvpn6PtSp"
      },
      "source": [
        "1. Plot the training and testing losses over epochs [2pt]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU_xNM1JPsmR"
      },
      "source": [
        "fig_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w-v2BvGPu1B"
      },
      "source": [
        "2. Plot the training and testing accuracies over epochs [2pt]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDmdXkrVPqwZ"
      },
      "source": [
        "fig_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ65cvaWPxYR"
      },
      "source": [
        "3. Print the final training and testing losses at convergence [2pt]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-vcO-FgPzoJ"
      },
      "source": [
        "result_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEEeO91oPz6x"
      },
      "source": [
        "4. Print the final training and testing accuracies at convergence [20pt]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMoFM6bUP0nB"
      },
      "source": [
        "result_acc"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}