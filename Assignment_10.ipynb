{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_10.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOyAErMcQfaekYw94R7Ni4s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yesoly/MachineLearningProject/blob/master/Assignment_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx7bjb1zOwMm"
      },
      "source": [
        "# Optimal Selection of the hyper-parameters associated with the classification on MNIST\n",
        "Choose an optimal set of hyper-parameters and design a neural network for the classification of MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXLdTm3EQLEm"
      },
      "source": [
        "import os\n",
        "\n",
        "# load data\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# train\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "\n",
        "# visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7EUEJRYRMMn"
      },
      "source": [
        "check device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp8BdsB8RH06"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device: {}'.format(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoLzwBZTO3_w"
      },
      "source": [
        "## 1. Data\n",
        "* you can use any data normalisation method\n",
        "* one example of the data normalisation is whitenning as given by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aAcS6JJO1MU"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,),(0.3081,)),  # mean value = 0.1307, standard deviation value = 0.3081\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVIk29VmPBl6"
      },
      "source": [
        "* load the MNIST dataset\n",
        "* use the original training dataset for testing your model\n",
        "* use the original testing dataset for training your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FESTOm1nOv7e"
      },
      "source": [
        "data_path = './MNIST'\n",
        "\n",
        "data_test   = datasets.MNIST(root = data_path, train= True, download=True, transform= transform)\n",
        "data_train  = datasets.MNIST(root = data_path, train= False, download=True, transform= transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEI8L3lIPIk1"
      },
      "source": [
        "* Note that the number of your training data must be 10,000\n",
        "* Note that the number of your testing data must be 60,000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwHjktlEOv1Q"
      },
      "source": [
        "print(\"the number of your training data (must be 10,000) = \", data_train.__len__())\n",
        "print(\"hte number of your testing data (must be 60,000) = \", data_test.__len__())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYeRoqwGPMvq"
      },
      "source": [
        "## 2. Model\n",
        "\n",
        "* design a neural network architecture with three layers (input layer, one hidden layer and output layer)\n",
        "* the input dimension of the input layer should be 784 (28 * 28)\n",
        "* the output dimension of the output layer should be 10 (class of digits)\n",
        "* all the layers should be fully connected layers\n",
        "* use any type of activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiBNYfzZOqM9"
      },
      "source": [
        "class classification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(classification, self).__init__()\n",
        "        \n",
        "        # construct layers for a neural network\n",
        "        self.classifier1 = nn.Sequential(\n",
        "            nn.Linear(in_features=28*28, out_features=dim_layer1_out),\n",
        "            nn.activation_layer1,\n",
        "        ) \n",
        "        self.classifier2 = nn.Sequential(\n",
        "            nn.Linear(in_features=dim_layer2_in, out_features=dim_layer2_out),\n",
        "            nn.activation_layer2,\n",
        "        ) \n",
        "        self.classifier3 = nn.Sequential(\n",
        "            nn.Linear(in_features=dim_layer3_in, out_features=10),\n",
        "            nn.activation_layer3,\n",
        "        ) \n",
        "    \n",
        "    def forward(self, inputs):                 # [batchSize, 1, 28, 28]\n",
        "        x = inputs.view(inputs.size(0), -1)    # [batchSize, 28*28]\n",
        "        x = self.classifier1(x)                # [batchSize, 20*20]\n",
        "        x = self.classifier2(x)                # [batchSize, 10*10]\n",
        "        out = self.classifier3(x)              # [batchSize, 10]\n",
        "        \n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUjvEAvoPXEd"
      },
      "source": [
        "## 3. Loss function\n",
        "* use any type of loss function\n",
        "* design the output of the output layer considering your loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28p5aYttPY4V"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4FsFtk9Pck9"
      },
      "source": [
        "## 4. Optimization\n",
        "* use any stochastic gradient descent algorithm for the optimization\n",
        "* use any size of the mini-batch\n",
        "* use any optimization algorithm (for example, Momentum, AdaGrad, RMSProp, Adam)\n",
        "* use any regularization algorithm (for example, Dropout, Weight Decay)\n",
        "* use any annealing scheme for the learning rate (for example, constant, decay, staircase)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVndxE3BSSUN"
      },
      "source": [
        "net = classification()\n",
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWxEtzMPPoeL"
      },
      "source": [
        "lr = 0.0001\n",
        "\n",
        "optimizer = optim.RMSprop(net.parameters(), lr=lr, alpha=0.9)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaUXG2qrR0Oc"
      },
      "source": [
        "##5. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1ewx1DdR2ok"
      },
      "source": [
        "def accuracy(out, y):\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds == y).float().mean().item(), len(y)\n",
        "\n",
        "\n",
        "def loss_batch(net, x, y):\n",
        "    loss = loss_fn(net(x), y)\n",
        "    return loss.item(), len(x)\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    print('Train model')\n",
        "    best_model_wts = copy.deepcopy(net.state_dict())\n",
        "    best_acc = 0.0\n",
        "    loss_show = []\n",
        "    acc_show = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_since = time.time()\n",
        "        print(\"Epoch {}:\".format(epoch), end=' ')\n",
        "        net.train()\n",
        "        for step, (b_x, b_y) in enumerate(train_dl):\n",
        "            out = net(b_x.to(device))\n",
        "            loss = loss_fn(out, b_y.to(device))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            losses, nums_loss = zip(\n",
        "                *[loss_batch(net, x.to(device), y.to(device)) for x, y in valid_dl]\n",
        "            )\n",
        "            acc, nums_acc = zip(\n",
        "                *[accuracy(net(x.to(device)), y.to(device)) for x, y in valid_dl]\n",
        "            )\n",
        "        val_loss = np.sum(np.multiply(losses, nums_loss)) / np.sum(nums_loss)\n",
        "        val_acc = np.sum(np.multiply(acc, nums_acc)) / np.sum(nums_acc)\n",
        "        epoch_time_elapsed = time.time() - epoch_since\n",
        "        print(\"time:{:.0f}s\".format(epoch_time_elapsed), \"loss:{:.10f}\".format(val_loss), \"accuracy:{:.10f}\".format(val_acc))\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(net.state_dict())\n",
        "        scheduler.step(val_loss)\n",
        "        loss_show.append(val_loss)\n",
        "        acc_show.append(val_acc)\n",
        "\n",
        "        if epoch % 10 == 9:\n",
        "            net.load_state_dict(best_model_wts)\n",
        "# torch.save(net.state_dict(), net_path)\n",
        "    net.load_state_dict(best_model_wts)\n",
        "# Save your model if you want\n",
        "# torch.save(net.state_dict(), net_path)\n",
        "    plt.figure()\n",
        "    plt.plot(range(epochs), loss_show)\n",
        "    plt.figure()\n",
        "    plt.plot(range(epochs), acc_show)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "\n",
        "# Set epoch to 30 to get 99.4% accuracy\n",
        "epochs = 10\n",
        "train_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i9TQxoTPouU"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOuVvpn6PtSp"
      },
      "source": [
        "1. Plot the training and testing losses over epochs [2pt]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU_xNM1JPsmR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w-v2BvGPu1B"
      },
      "source": [
        "2. Plot the training and testing accuracies over epochs [2pt]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDmdXkrVPqwZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ65cvaWPxYR"
      },
      "source": [
        "3. Print the final training and testing losses at convergence [2pt]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-vcO-FgPzoJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEEeO91oPz6x"
      },
      "source": [
        "4. Print the final training and testing accuracies at convergence [20pt]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMoFM6bUP0nB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}